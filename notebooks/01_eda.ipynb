{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual App Reviews - Exploratory Data Analysis\n",
    "\n",
    "This notebook provides an initial exploration of the multilingual mobile app reviews dataset. We'll examine the data structure, identify patterns, and prepare for deeper analysis.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading](#data-loading)\n",
    "2. [Dataset Overview](#dataset-overview)\n",
    "3. [Data Quality Assessment](#data-quality-assessment)\n",
    "4. [Initial Insights](#initial-insights)\n",
    "5. [Next Steps](#next-steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "First, let's load the raw dataset from the CSV file. Make sure you have placed the `multilingual_mobile_app_reviews_2025.csv` file in the `../data/raw/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "project_root = Path().resolve().parent  # Go up one level from notebooks/\n",
    "raw_data_path = project_root / \"data\" / \"raw\"\n",
    "csv_file = raw_data_path / \"multilingual_mobile_app_reviews_2025.csv\"\n",
    "\n",
    "print(f\"Looking for dataset at: {csv_file}\")\n",
    "print(f\"File exists: {csv_file.exists()}\")\n",
    "\n",
    "# Load the dataset\n",
    "if csv_file.exists():\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "        print(f\"‚úÖ Successfully loaded dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"UTF-8 encoding failed, trying latin-1...\")\n",
    "        df = pd.read_csv(csv_file, encoding='latin-1')\n",
    "        print(f\"‚úÖ Successfully loaded dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset file not found!\")\n",
    "    print(\"Please download 'multilingual_mobile_app_reviews_2025.csv' from Kaggle\")\n",
    "    print(\"and place it in the data/raw/ directory.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "Let's examine the basic structure of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üìä DATASET SHAPE\")\n",
    "    print(f\"Rows: {df.shape[0]:,}\")\n",
    "    print(f\"Columns: {df.shape[1]:,}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nüìã COLUMN INFORMATION\")\n",
    "    print(\"Column names and data types:\")\n",
    "    for i, (col, dtype) in enumerate(df.dtypes.items()):\n",
    "        print(f\"{i+1:2d}. {col:<30} | {str(dtype):<15}\")\n",
    "    \n",
    "    print(\"\\nüîç FIRST FEW ROWS\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "Now let's check for missing values and data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"üîç MISSING VALUES ANALYSIS\")\n",
    "    missing_info = df.isnull().sum()\n",
    "    missing_info = missing_info[missing_info > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(missing_info) > 0:\n",
    "        print(\"Columns with missing values:\")\n",
    "        for col, count in missing_info.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {col:<30}: {count:,} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found!\")\n",
    "    \n",
    "    print(f\"\\nüìà BASIC STATISTICS\")\n",
    "    print(\"Numeric columns summary:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(f\"\\nüìù TEXT COLUMNS SAMPLE\")\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols[:3]:  # Show first 3 text columns\n",
    "        print(f\"\\n{col} - Sample values:\")\n",
    "        unique_vals = df[col].dropna().unique()[:5]\n",
    "        for val in unique_vals:\n",
    "            print(f\"  ‚Ä¢ {str(val)[:100]}...\")  # Truncate long text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Insights\n",
    "\n",
    "Let's generate some quick visualizations to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Try to identify key columns\n",
    "    rating_col = None\n",
    "    language_col = None\n",
    "    app_col = None\n",
    "    date_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if 'rating' in col.lower() or 'score' in col.lower():\n",
    "            rating_col = col\n",
    "        if 'lang' in col.lower():\n",
    "            language_col = col\n",
    "        if 'app' in col.lower():\n",
    "            app_col = col\n",
    "        if 'date' in col.lower() or 'time' in col.lower():\n",
    "            date_col = col\n",
    "    \n",
    "    print(f\"üéØ IDENTIFIED KEY COLUMNS\")\n",
    "    print(f\"Rating column: {rating_col}\")\n",
    "    print(f\"Language column: {language_col}\")\n",
    "    print(f\"App column: {app_col}\")\n",
    "    print(f\"Date column: {date_col}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Initial Data Exploration', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Rating distribution (if rating column exists)\n",
    "    if rating_col and df[rating_col].notna().sum() > 0:\n",
    "        axes[0, 0].hist(df[rating_col].dropna(), bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].set_title(f'Distribution of {rating_col}')\n",
    "        axes[0, 0].set_xlabel('Rating')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'No rating data\\navailable', \n",
    "                       ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "        axes[0, 0].set_title('Rating Distribution')\n",
    "    \n",
    "    # Plot 2: Language distribution (if language column exists)\n",
    "    if language_col and df[language_col].notna().sum() > 0:\n",
    "        lang_counts = df[language_col].value_counts().head(10)\n",
    "        axes[0, 1].bar(range(len(lang_counts)), lang_counts.values)\n",
    "        axes[0, 1].set_title(f'Top 10 Languages')\n",
    "        axes[0, 1].set_xlabel('Language')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "        axes[0, 1].set_xticks(range(len(lang_counts)))\n",
    "        axes[0, 1].set_xticklabels(lang_counts.index, rotation=45, ha='right')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No language data\\navailable', \n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Language Distribution')\n",
    "    \n",
    "    # Plot 3: Missing values heatmap\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        missing_data = missing_data[missing_data > 0]\n",
    "        axes[1, 0].bar(range(len(missing_data)), missing_data.values)\n",
    "        axes[1, 0].set_title('Missing Values by Column')\n",
    "        axes[1, 0].set_xlabel('Column')\n",
    "        axes[1, 0].set_ylabel('Missing Count')\n",
    "        axes[1, 0].set_xticks(range(len(missing_data)))\n",
    "        axes[1, 0].set_xticklabels(missing_data.index, rotation=45, ha='right')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No missing\\nvalues found!', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes,\n",
    "                       fontsize=14, color='green', fontweight='bold')\n",
    "        axes[1, 0].set_title('Missing Values')\n",
    "    \n",
    "    # Plot 4: Data types distribution\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    axes[1, 1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Data Types Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä QUICK STATS SUMMARY\")\n",
    "    print(f\"‚Ä¢ Total reviews: {len(df):,}\")\n",
    "    if rating_col:\n",
    "        valid_ratings = df[rating_col].dropna()\n",
    "        if len(valid_ratings) > 0:\n",
    "            print(f\"‚Ä¢ Average rating: {valid_ratings.mean():.2f}\")\n",
    "            print(f\"‚Ä¢ Rating range: {valid_ratings.min():.1f} - {valid_ratings.max():.1f}\")\n",
    "    if language_col:\n",
    "        unique_langs = df[language_col].nunique()\n",
    "        print(f\"‚Ä¢ Unique languages: {unique_langs}\")\n",
    "    if app_col:\n",
    "        unique_apps = df[app_col].nunique()\n",
    "        print(f\"‚Ä¢ Unique apps: {unique_apps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on this initial exploration, here are the recommended next steps:\n",
    "\n",
    "1. **Data Cleaning**: Run the data preprocessing script to handle missing values and standardize data types\n",
    "2. **Language Detection**: For any missing language labels, use automatic language detection\n",
    "3. **Detailed Analysis**: Use the analytics script to generate comprehensive insights and visualizations\n",
    "4. **Advanced Analytics**: Consider sentiment analysis, topic modeling, or trend analysis\n",
    "\n",
    "### Running the Analysis Pipeline\n",
    "\n",
    "```bash\n",
    "# Clean and preprocess the data\n",
    "python src/data_prep.py --verbose\n",
    "\n",
    "# Generate analytics and visualizations  \n",
    "python src/analytics.py --verbose\n",
    "```\n",
    "\n",
    "The cleaned data will be saved to `data/processed/` and visualizations to `reports/figures/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
